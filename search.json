[{"title":"调式和调性","path":"/2025/07/20/调式、调性/","content":"调式和调性调式 “调式”是音乐中的一个基础概念，本质上是音与音之间关系的组织方式。它决定了一段音乐的音阶结构、情绪倾向和和声风格。 调性 调性：音乐的组织原则 —— 把某个音（主音）当作重心，形成一种主从关系的系统。","tags":["音乐","乐理","调式","调性"],"categories":["音乐","乐理"]},{"title":"拍子","path":"/2025/07/08/拍子/","content":"拍子单拍子、复拍子单拍子 复拍子 混合拍子 一拍子 X拍子 散拍子 变换拍子、交错拍子","tags":["音乐","乐理","拍子"],"categories":["音乐","乐理"]},{"path":"/2025/06/26/休止符、强弱关系/","content":"休止符、强弱关系休止符 休止符表示方式 常见的拍号强弱关系"},{"title":"一拍、小节、小节线、终止线、附点、延音线","path":"/2025/06/26/一拍/","content":"一拍、小节、小节线、终止线、附点、延音线什么叫一拍？ 举例 X&#x2F;4 : 以四分音符（的时长）为一拍，每小节有X拍（的时长）。 X&#x2F;8 : 以八分音符（的时长）为一拍，每小节有X拍（的时长）。 读谱 附点 延长音符本身时值的一半。 复附点 延长音符前一个音时值的一半。 延音线 用来组合无法用常规方式表达的拍数。 某个音符跨越小节线的时候。 必须连接相同的音。 音值组合法中的使用。 连线 歌谱中，某一个字对应几个音的时候，需要用连线将这几个音连接起来。 框起来的部分，要唱（奏）得连贯。 延音线 和 连线 的区别","tags":["音乐","乐理","一拍","小节","小节线","终止线","附点","延音线"],"categories":["音乐","乐理"]},{"title":"权重衰减","path":"/2025/06/15/权重衰减/","content":"权重衰减 机器学习中的正则化技术，尤其是 ​L2正则化（权重衰减）的原理和作用。 要解决的问题 过拟合困境​ 模型复杂度过高时会过度拟合训练数据（如高阶多项式回归），导致在新数据上表现差。 数据收集的局限性 获取更多训练数据可缓解过拟合，但成本高、耗时长或不可控。 特征数量的矛盾 通过简单限制特征数量（比如在多项式回归中控制模型的阶数）来控制模型复杂度，但这种做法太生硬 —— 模型要么太简单（欠拟合），要么太复杂（过拟合），缺乏平滑的过渡。 总之：需要更精细的工具平衡模型复杂度。 解决方案：权重衰减（L2正则化）1. 核心思想 ​用权重向量的范数衡量模型复杂度​： 修改损失函数​： 2. 为什么用L2范数 3. ​权重衰减的更新规则（梯度下降）​​ 实践细节 总结","tags":["pytorch","权重衰减"],"categories":["人工智能"]},{"title":"原调、移调、转调、离调","path":"/2025/06/13/原调、移调、转调、离调/","content":"原调、移调、转调、离调 原调(名词) 原来的调，没有变化。 移调(动词) 向上升了几个，或者向下降了几个。 转调(动词) 原来的调，转调另外一个调，并 在另外一个调结束 。 离调(动词) 原来的调，转调另外一个调，又转回来，并 在原来的调结束 。 简谱需要移调吗？ 1 = D/C 那么 do 就一定是 D/C了！！所以简谱一般不移调。 高八度或者低八度不算跑调","tags":["音乐","乐理","原调","移调","转调","离调"],"categories":["音乐","乐理"]},{"title":"首调和固定调","path":"/2025/06/13/首调和固定调/","content":"首调和固定调首调 首调：第一个音都唱 1 do 固定调 固定调：每个音名都是原来的音 应用场景 常常 首调用来唱，固定调用来弹。","tags":["音乐","乐理","首调","固定调"],"categories":["音乐","乐理"]},{"title":"作业_250613","path":"/2025/06/13/音乐基本功_250613/","content":"250613找等音 bb是降不是d","tags":["音乐","作业_250613"],"categories":["音乐","作业"]},{"title":"安装d2l出错","path":"/2025/06/09/安装d2l包出错/","content":"安装d2l出错发生原因 输入： 1pip install d2l -i https://pypi.tuna.tsinghua.edu.cn/simple/ 报错： 原因 ​Python 3.12 不兼容性​： Python 3.12 移除了 pkgutil.ImpImporter 功能（错误中出现 AttributeError: module ‘pkgutil’ has no attribute ‘ImpImporter’） NumPy 1.23.5 及以下版本尚未适配 Python 3.12 ​d2l 依赖冲突​： d2l 包要求特定版本的 NumPy (1.23.5)，该版本无法在 Python 3.12 上编译安装。 解决方法1、 初始化conda 12345678# 对于 zsh 用户conda init zsh# 对于 bash 用户conda init bash# 如果不知道 shell 类型，尝试通用初始化conda init 2、 启动一个新终端会话，关闭当前终端窗口，打开一个新的终端窗口以使初始化生效。3、 验证 Conda 初始化 1conda info 4、 创建并激活新环境 12345# 创建 Python 3.10 环境（解决之前的兼容性问题）conda create -n d2l_env python=3.10# 激活环境conda activate d2l_env 5、 安装 d2l 和其他必要包 1pip install d2l numpy matplotlib pandas jupyter -i https://pypi.tuna.tsinghua.edu.cn/simple/ 6、 验证 12import d2lprint(f&quot;d2l version: &#123;d2l.__version__&#125;&quot;) 7、 进入jupyter notebook 里面选择环境 注意 在 jupyter notebook 里面需要输入 y 的时候，直接输入 –yes 就行。如： 1conda create -n d2l_env python=3.10 --yes","tags":["python","d2l"],"categories":["人工智能"]},{"title":"黑键出发的自然大调、等音调","path":"/2025/06/06/黑键出发的自然大调、等音调/","content":"黑键出发的自然大调、等音调黑键出发的自然大调 遵循规则：全全半全全全半 以及 CDEFGAB 要全部出现。 等音调","tags":["音乐","乐理","黑键出发的自然大调","等音调"],"categories":["音乐","乐理"]},{"title":"调式、自然大调式","path":"/2025/06/06/调式、自然大调式/","content":"调式、自然大调式、调性调式 若干高低不同的乐音，围绕某一有稳定感的中心音，按一定的音程关系组织在一起，成为一个有机的体系，称为调式。 简单理解：若干个音，按某种规则排列起来，就是调式。 自然大调式 遵循 全全半全全全半 规则排列。 音阶 什么叫调性？ 调性 &#x3D; 主音 + 调式 (主音、调式都要相同)","tags":["音乐","乐理","调式","自然大调式"],"categories":["音乐","乐理"]},{"title":"自然半音、变化半音、自然全音、变化全音","path":"/2025/06/06/自然半音、变化半音、自然全音/","content":"自然半音、变化半音、自然全音、变化全音自然半音、自然全音 两个相邻的音，比如C和D、E和F，只能是相邻，如果是相同也叫变化。 如果原来的音差半个就是半音，差一个音就是全音。 变化半音、变化全音 变化就是去掉升降号后，这两个音名没有挨着，哪怕一样也是变化。 判断方法","tags":["音乐","乐理","自然半音","变化半音","自然全音","变化全音"],"categories":["音乐","乐理"]},{"title":"1234567的内在规则","path":"/2025/06/05/1234567的内在规则/","content":"1234567的内在规则 全全半全全全半","tags":["音乐","乐理","内在规则"],"categories":["音乐","乐理"]},{"title":"音级、基本音级、变化音级","path":"/2025/06/05/音级、基本音级、变化音级/","content":"音级、基本音级、变化音级音级 乐音体系中的每一个音，都叫音级。 1是整敬，2是整敬，3是整数。 C是音级，D是音级，E是音级。 基本音级、变化音级 变化音级：有升降号的就是变化音级。","tags":["音乐","乐理","音级","基本音级","变化音级"],"categories":["音乐","乐理"]},{"title":"乐音、噪音、乐音体系、音列","path":"/2025/06/05/乐音、噪音、乐音体系、音列/","content":"乐音、噪音、乐音体系、音列乐音、噪音 打击乐器算作“噪音乐器”。 乐音 ：振动规则，频逐稳定。 噪音 ：振动不规则，频率不稳定。 音乐往往是两者的结合！！ 乐音体系 音乐中使用到的所有乐音的总和: 88 &#x2F; 97 。 音列 乐音体系中，取若干乐音，并有序地排列起来，称为 音列。","tags":["音乐","乐理","乐音","噪音","乐音体系","音列"],"categories":["音乐","乐理"]},{"title":"音的分组、音域、半音、全音","path":"/2025/06/04/音的分组、中央C、标准音/","content":"音的分组、音域、半音、全音音的分组 音域 半音 所有乐器都是一样： 全音","tags":["音乐","乐理","音的分组","音域","半音","全音"],"categories":["音乐","乐理"]},{"title":"删除列&&数据集转换为张量","path":"/2025/06/03/作业：删除缺失值列以及转换为张量格式/","content":"作业 2.1.11、 删除缺失值最多的列步骤1、 计算每列的缺失值数量123missing_counts = inputs.isnull().sum()print(&quot;每列缺失值数量:&quot;)print(missing_counts) 2、 找到缺失值最多的列名123# 找到缺失值最多的列名max_missing_column = missing_counts.idxmax()print(f&quot; 缺失值最多的列: &#123;max_missing_column&#125;&quot;) 3、 删除该列1234# 删除该列data = data.drop(max_missing_column, axis=1)print(&quot; 删除后的数据:&quot;)print(data) 2、 将预处理后的数据集转换为张量格式12345678910111213141516import torch# 将DataFrame转换为PyTorch张量X = torch.tensor(inputs.values.astype(float))# 分离输出标签y = torch.tensor(outputs.values.astype(float))print(&quot; 输入张量X：&quot;)print(X)print(&quot;形状:&quot;, X.shape)print(&quot; 输出张量y：&quot;)print(y)print(&quot;形状:&quot;, y.shape)","tags":["人工智能","作业"],"categories":["人工智能"]},{"title":"处理缺失值报错","path":"/2025/06/01/处理缺失值报错/","content":"处理缺失值报错原因故障 代码123inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]inputs = inputs.fillna(inputs.mean())print(inputs) 报错1TypeError: can only concatenate str (not &quot;int&quot;) to str 原因​inputs.fillna(inputs.mean()) 仅适用于数值列，分类列需单独处理。 处理用 inputs.NumRooms 选中数值列。 123inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]inputs.NumRooms.fillna(inputs.NumRooms.mean())print(inputs)","tags":["pytorch","处理缺失值"],"categories":["人工智能"]},{"title":"调号","path":"/2025/05/30/调号/","content":"调号 拍号 拍号就好像律动，但不等于律动，先这样理解。 情绪与速度情绪 用心感受情绪，并唱出情绪。 速度 抽象的就是一个大概的速度，比如120-140之间就行，但精确的130就必须130。 1(do)的位置一旦确定，234567的位置就确定了 黑键的表示 纯八度即 1:2 就可以 什么叫调性？ 调性 &#x3D; 主音 + 调式","tags":["音乐","乐理","调号"],"categories":["音乐","乐理"]},{"title":"数据预处理","path":"/2025/05/29/数据预处理/","content":"数据预处理 在Python中常用的数据分析工具中，我们通常使用pandas软件包。 读取数据集 首先创建一个人工数据集，并存储在CSV（逗号分隔值）文件 ..&#x2F;data&#x2F;house_tiny.csv中。 1234567891011121314151617181920# 导入操作系统接口模块，用于处理文件和目录路径import os# 创建目录：在上级目录(&#x27;..&#x27;)中创建名为&#x27;data&#x27;的子目录# exist_ok=True 表示如果目录已存在不会抛出错误os.makedirs(os.path.join(&#x27;..&#x27;, &#x27;data&#x27;), exist_ok=True)# 定义文件路径：构建完整文件路径 &#x27;../data/house_tiny.csv&#x27;data_file = os.path.join(&#x27;..&#x27;, &#x27;data&#x27;, &#x27;house_tiny.csv&#x27;)# 创建并写入CSV文件with open(data_file, &#x27;w&#x27;) as f: # 以写入模式打开文件 # 写入列标题（特征说明） f.write(&#x27;NumRooms,Alley,Price &#x27;) # 房间数量，巷子类型，房屋价格 # 写入四个数据样本（包含缺失值NA） f.write(&#x27;NA,Pave,127500 &#x27;) # 样本1：房间数缺失，巷子类型为铺砌，价格12.75万 f.write(&#x27;2,NA,106000 &#x27;) # 样本2：2个房间，巷子类型缺失，价格10.6万 f.write(&#x27;4,NA,178100 &#x27;) # 样本3：4个房间，巷子类型缺失，价格17.81万 f.write(&#x27;NA,NA,140000 &#x27;) # 样本4：房间数和巷子类型均缺失，价格14万 从创建的CSV文件中加载原始数据集，我们导入pandas包并调用read_csv函数。 123456# 如果没有安装pandas，只需取消对以下行的注释来安装pandas# !pip install pandasimport pandas as pddata = pd.read_csv(data_file)print(data) 12345NumRooms Alley Price 0 NaN Pave 127500 1 2.0 NaN 106000 2 4.0 NaN 178100 3 NaN NaN 140000 处理缺失值 “NaN”项代表缺失值。 处理方法： 插值法 用一个替代值弥补缺失值。 删除法 直接忽略缺失值。 举例：插值 处理前 描述：通过位置索引iloc，我们将data分成inputs和outputs， 其中前者为data的前两列，而后者为data的最后一列。 对于inputs中缺少的数值，我们用同一列的均值替换“NaN”项。 123inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]inputs = inputs.fillna(inputs.mean())print(inputs) 12345 NumRooms Alley0 3.0 Pave1 2.0 NaN2 4.0 NaN3 3.0 NaN 处理后 12345678910111213141516# 使用 pandas 的 get_dummies 方法对分类特征进行独热编码（One-Hot Encoding）# 参数说明：# - dummy_na=True: 将缺失值（NaN）也视为一个独立类别进行编码# 操作结果：# - 对 inputs DataFrame 中所有对象类型（object）或分类（category）的列生成哑变量# - 每列的不同取值（包括 NaN）将被扩展为新的二进制列（0/1 表示是否存在该类别）inputs = pd.get_dummies(inputs, dummy_na=True)# 打印处理后的数据# 输出示例：# 原始列 Alley 包含 [&#x27;Pave&#x27;, &#x27;Gravel&#x27;, NaN] 三类，转换后将生成三列：# - Alley_Pave（是否为铺砌道路）# - Alley_Gravel（是否为碎石道路）# - Alley_nan（是否存在道路类型缺失）print(inputs) 12345 NumRooms Alley_Pave Alley_nan0 3.0 1 01 2.0 0 12 4.0 0 13 3.0 0 1 描述： 对于inputs中的类别值或离散值，我们将“NaN”视为一个类别。 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”， pandas可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。 缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。 转换成张量格式 inputs和outputs中的所有条目都是数值类型，它们可以转换为张量格式。 1234import torchX = torch.tensor(inputs.to_numpy(dtype=float))y = torch.tensor(outputs.to_numpy(dtype=float))X, y 123456 (tensor([ [3., 1., 0.], [2., 0., 1.], [4., 0., 1.], [3., 0., 1.]], dtype=torch.float64),tensor([127500., 106000., 178100., 140000.], dtype=torch.float64))","tags":["pytorch","Dive into deep learning","数据操作","数据预处理"],"categories":["人工智能"]},{"title":"Pytorch数据操作","path":"/2025/05/29/数据操作/","content":"数据操作入门 n维数组，也称为 张量（tensor） 。 无论使用哪个深度学习框架，它的张量类（在MXNet中为ndarray， 在PyTorch和TensorFlow中为Tensor）都与Numpy的ndarray类似。 但深度学习框架又比Numpy的ndarray多一些重要功能： 首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； 其次， 张量类支持自动微分 。这些功能使得张量类更适合深度学习。如果没有特殊说明，本书中所说的张量均指的是张量类的实例。 导入torch，它被称为PyTorch，但是代码中使用torch而不是pytorch。 1import torch 张量表示一个由数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的向量（vector）； 具有两个轴的张量对应数学上的矩阵（matrix）； 具有两个轴以上的张量没有特殊的数学名称。 我们可以使用 arange 创建一个行向量x。 这个行向量包含以0开始的前12个整数，它们默认创建为整数。也 可指定创建类型为浮点数。 张量中的每个值都称为张量的元素（element）。 12x = torch.arange(12)x 1tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) 张量的shape属性来访问张量（沿每个轴的长度）的形状。 1x.shape 1torch.Size([12]) 只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。 12345 x.numel()``` ```python 12 要想改变一个张量的形状而不改变元素数量和元素值，可以调用reshape函数。 我们可以通过-1来调用此自动计算出维度的功能。 即我们可以用x.reshape(-1,4)或x.reshape(3,-1)来取代x.reshape(3,4)。 总元素数不变​：张量在 reshape 后元素总数必须与原始张量一致。 12X = x.reshape(3, 4)X 1234tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) 希望使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。 我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。 1torch.zeros((2, 3, 4)) 12345678tensor([[[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]],[[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]]) 可以创建一个形状为(2,3,4)的张量，其中所有元素都设置为1。 1torch.ones((2, 3, 4)) 12345678tensor([[[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]],[[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]]) 从某个特定的概率分布中随机采样来得到张量中每个元素的值。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。 1torch.randn(3, 4) 123tensor([[-0.0135, 0.0665, 0.0912, 0.3212], [ 1.4653, 0.1843, -1.6995, -0.3036], [ 1.7646, 1.0450, 0.2457, -0.7732]]) 我们还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。 1torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) 123tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) 运算符 最简单且最有用的操作是 按元素（elementwise）运算 。 它们将标准标量运算符 应用于数组的每个元素 。 对于将两个数组作为输入的函数，按元素运算将二元运算符应用于两个数组中的每对位置对应的元素。 常见的标准算术运算符（+、-、*、&#x2F;和**）都可以被升级为按元素运算。 我们可以在 同一形状 的任意两个张量上调用按元素操作。 123x = torch.tensor([1.0, 2, 4, 8])y = torch.tensor([2, 2, 2, 2])x + y, x - y, x * y, x / y, x ** y # **运算符是求幂运算 12345(tensor([ 3., 4., 6., 10.]),tensor([-1., 0., 2., 6.]),tensor([ 2., 4., 8., 16.]),tensor([0.5000, 1.0000, 2.0000, 4.0000]),tensor([ 1., 4., 16., 64.])) “按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。 1torch.exp(x) 1tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03]) 把多个张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 只需要提供张量列表，并给出沿哪个轴连结。 123X = torch.arange(12, dtype=torch.float32).reshape((3,4))Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1) 123456789(tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [ 2., 1., 4., 3.], [ 1., 2., 3., 4.], [ 4., 3., 2., 1.]]),tensor([[ 0., 1., 2., 3., 2., 1., 4., 3.], [ 4., 5., 6., 7., 1., 2., 3., 4.], [ 8., 9., 10., 11., 4., 3., 2., 1.]])) 通过逻辑运算符构建二元张量。 以X &#x3D;&#x3D; Y为例： 对于每个位置，如果X和Y在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句X &#x3D;&#x3D; Y在该位置处为真，否则该位置为0。 1X == Y 123tensor([[False, True, False, True], [False, False, False, False], [False, False, False, False]]) 对张量中的所有元素进行求和，会产生一个单元素张量。 1X.sum() 1tensor(66.) 广播机制 在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism） 来执行按元素操作。 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状； 对生成的数组执行按元素操作。 在大多数情况下，我们将沿着数组中长度为1的轴进行广播。 123a = torch.arange(3).reshape((3, 1))b = torch.arange(2).reshape((1, 2))a, b 1234(tensor([[0], [1], [2]]),tensor([[0, 1]])) 由于a和b分别是 3X1 和 1X2 矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵广播为一个更大的矩阵，如下所示：矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。 1a + b 123tensor([[0, 1], [1, 2], [2, 3]]) 索引和切片 张量中的元素可以通过索引访问。 第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。 1X[-1], X[1:3] 123(tensor([ 8., 9., 10., 11.]), tensor([[ 4., 5., 6., 7.], [ 8., 9., 10., 11.]])) 还可以通过指定索引来将元素写入矩阵。 12X[1, 2] = 9X 123tensor([[ 0., 1., 2., 3.], [ 4., 5., 9., 7.], [ 8., 9., 10., 11.]]) 如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。 例如，[0:2, :]访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。 12X[0:2, :] = 12X 123tensor([[12., 12., 12., 12.], [12., 12., 12., 12.], [ 8., 9., 10., 11.]]) 节省内存 运行一些操作可能会导致为新结果分配内存。 运行 Y &#x3D; Y + X 后，我们会发现id(Y)指向另一个位置。这是因为Python首先计算 Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置。 123before = id(Y)Y = Y + Xid(Y) == before 1False 不可取的，原因: 首先，我们不想总是不必要地分配内存。 如果我们不原地更新，其他引用仍然会指向旧的内存位置， 解决： 可以使用切片表示法将操作的结果分配给先前分配的数组，例如 Y[:] &#x3D; &lt;expression&gt;。 1234Z = torch.zeros_like(Y)print(&#x27;id(Z):&#x27;, id(Z))Z[:] = X + Yprint(&#x27;id(Z):&#x27;, id(Z)) 12id(Z): 140327634811696id(Z): 140327634811696 在后续计算中没有重复使用X， 我们也可以使用 X[:] &#x3D; X + Y 或 X +&#x3D; Y 来减少操作的内存开销。 123before = id(X)X += Yid(X) == before 1True 转换成其他Python对象 将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存 ，就地操作更改一个张量也会同时更改另一个张量。 123A = X.numpy()B = torch.tensor(A)type(A), type(B) 1(numpy.ndarray, torch.Tensor) 要将大小为1的张量转换为Python标量，我们可以调用 item函数 或 Python的内置函数 。 12a = torch.tensor([3.5])a, a.item(), float(a), int(a) 1(tensor([3.5000]), 3.5, 3.5, 3)","tags":["pytorch","Dive into deep learning","数据操作"],"categories":["人工智能"]},{"title":"初识深度学习","path":"/2025/05/29/深度学习/","content":"初识深度学习起源 神经网络（neural networks）其核心是当今大多数网络中都可以找到的几个关键原则： 线性和非线性处理单元的交替，通常称为层（layers）； 使用链式规则（也称为反向传播（backpropagation））一次性调整网络中的全部参数。 发展 允许轻松建模的第一代框架包括Caffe、Torch和Theano。许多开创性的论文都是用这些工具写的。到目前为止，它们已经被TensorFlow（通常通过其高级API Keras使用）、CNTK、Caffe 2和Apache MXNet所取代。第三代工具，即用于深度学习的命令式工具，可以说是由Chainer率先推出的，它使用类似于Python NumPy的语法来描述模型。这个想法被PyTorch、MXNet的Gluon API和Jax都采纳了。 特点 由于表示学习（representation learning）目的是 寻找表示本身 ，因此深度学习可以称为“多级表示学习”。 深度学习的一个关键优势是它不仅取代了传统学习管道末端的浅层模型，而且还取代了劳动密集型的特征工程过程。 通过取代大部分特定领域的预处理，深度学习消除了以前分隔计算机视觉、语音识别、自然语言处理、医学信息学和其他应用领域的许多界限，为解决各种问题提供了一套统一的工具。 除了端到端的训练，人们正在经历从参数统计描述到完全非参数模型的转变。 当数据稀缺时，人们需要依靠简化对现实的假设来获得有用的模型。 当数据丰富时，可以用更准确地拟合实际情况的非参数模型来代替。 小结 机器学习研究计算机系统如何利用经验（通常是数据）来提高特定任务的性能。 表示学习作为机器学习的一类，其研究的重点是如何自动找到合适的数据表示方式。 深度学习不仅取代了传统机器学习的浅层模型，而且取代了劳动密集型的特征工程。 最近在深度学习方面取得的许多进展，大都是由廉价传感器和互联网规模应用所产生的大量数据，以及（通过GPU）算力的突破来触发的。 整个系统优化是获得高性能的关键环节。","tags":["人工智能","机器学习","深度学习"],"categories":["人工智能"]},{"title":"各种机器学习问题","path":"/2025/05/27/各种机器学习问题/","content":"各种机器学习问题监督学习 监督学习（supervised learning） 擅长在“给定输入特征”的情况下 预测标签 ，每个“特征-标签”对都称为一个样本（example）。即使标签是未知的，样本也可以指代输入特征。—— 每个样本包含特征和相应标签值。 目标：生成一个模型，能够将任何输入特征映射到标签（即预测）。 监督学习之所以能发挥作用，是因为在训练参数时，我们为模型提供了一个数据集，其中每个样本都有真实的标签。 用概率论术语来说，我们希望预测 “估计给定输入特征的标签”的条件概率 。 监督学习只是几大类机器学习问题之一。 监督学习的学习过程(如下图) 从已知大量数据样本中随机选取一个子集，为每个样本获取真实标签。 有时，这些样本已有标签（例如，患者是否在下一年内康复？）。 有时，这些样本可能需要被人工标记（例如，图像分类）。 这些输入和相应的标签一起构成了训练数据集。 选择有监督的学习算法，它将训练数据集作为输入，并输出一个“已完成学习的模型”。 将之前没有见过的样本特征放到这个“已完成学习的模型”中，使用模型的输出作为相应标签的预测。 回归（regression） 判断回归问题的一个很好的经验法则是，任何有关 “有多少”的问题 很可能就是回归问题。比如： 这个手术需要多少小时； 在未来6小时，这个镇会有多少降雨量。 分类 希望模型能够 预测样本属于哪个类别（category，正式称为类（class）） 。 这款应用程序能够自动理解从图像中看到的文本，并将手写字符映射到对应的已知字符之上。 这种 “哪一个”的问题叫做分类（classification） 问题。 二项分类（binomial classification） 最简单的分类问题是只有两类。 例如，手写数字可能有10类，标签被设置为数字0～9。 数据集可能由动物图像组成，标签可能是{猫,狗}两类。 回归是训练一个回归函数来输出一个数值；分类是训练一个分类器来输出预测的类别。 比如，之前的猫狗分类例子中，分类器可能会输出图像是猫的概率为0.9。 0.9这个数字表达什么意思呢？可以这样理解：分类器90%确定图像描绘的是一只猫。预测类别的概率的大小传达了一种模型的不确定性，本书后面章节将讨论其他运用不确定性概念的算法。 多项分类（multiclass classification） 问题 常见的例子包括手写字符识别{1,2,3,…,8,9,a,b,c,…}与解决回归问题不同，分类问题的常见损失函数被称为 交叉熵（cross-entropy） 。 分类可能变得比二项分类、多项分类复杂得多。 有一些分类任务的变体可以用于寻找层次结构，层次结构假定在许多类之间存在某种关系。因此，并不是所有的错误都是均等的。人们宁愿错误地分入一个相关的类别，也不愿错误地分入一个遥远的类别，这通常被称为 层次分类(hierarchical classification) 。 例如 ： 良性错误​：将狮子(Panthera leo)误判为老虎(Panthera tigris)（同属Panthera，共享90%以上基因）；恶性错误​：将狮子误判为蝴蝶（跨越界级分类，基因相似度趋近于零）。 把一只狮子狗误认为雪纳瑞可能不会太糟糕。但如果模型将狮子狗与恐龙混淆，就滑稽至极了。层次结构相关性可能取决于模型的使用者计划如何使用模型。例如，响尾蛇和乌梢蛇血缘上可能很接近，但如果把响尾蛇误认为是乌梢蛇可能会是致命的。 因为响尾蛇是有毒的，而乌梢蛇是无毒的。 ​代价敏感学习（Cost-Sensitive Learning）：对致命错误设置更高惩罚权重（如蛇类毒性误判增加1000倍损失）。 标记问题 学习预测不相互排斥的类别的问题称为多标签分类（multi-label classification）。 比如“机器学习”“技术”“小工具”“编程语言”“Linux”“云计算”“AWS”。 一篇典型的文章可能会用5～10个标签，因为这些概念是相互关联的。 搜索 息检索领域，对一组项目进行排序。 方案：首先为集合中的每个元素分配相应的相关性分数，然后检索评级最高的元素。 推荐系统 目标：是向特定用户进行“个性化”推荐。 推荐系统会为“给定用户和物品”的匹配性打分，这个“分数”可能是估计的评级或购买的概率。 序列学习 序列学习需要摄取输入序列或预测输出序列，或两者兼而有之。 如果输入是连续的，模型可能就需要拥有“记忆”功能。 比如，在医学上序列输入和输出就更为重要。 设想一下，假设一个模型被用来监控重症监护病人，如果他们在未来24小时内死亡的风险超过某个阈值，这个模型就会发出警报。 我们绝不希望抛弃过去每小时有关病人病史的所有信息，而仅根据最近的测量结果做出预测。 虽然不可能考虑所有类型的序列转换，但以下特殊情况值得一提。 标记和解析 目标： 基于结构和语法假设对文本进行分解和注释，以获得一些注释。 自动语音识别 输入序列是说话人的录音，输出序列是说话人所说内容的文本记录。 挑战在于，与文本相比，音频帧多得多（声音通常以8kHz或16kHz采样）,音频和文本之间没有1:1的对应关系，因为数千个样本可能对应于一个单独的单词。 这是“序列到序列”的学习问题，其中输出比输入短得多。 文本到语音 输入是文本，输出是音频文件，输出比输入长得多。 机器翻译 在语音识别中，输入和输出的出现顺序基本相同。 而在机器翻译中，颠倒输入和输出的顺序非常重要。 无监督学习 数据中不含有“目标”的机器学习问题通常被为 无监督学习（unsupervised learning） —— 如果工作没有十分具体的目标，就需要“自发”地去学习了。—— 老板可能会给我们一大堆数据，然后要求用它做一些数据科学研究，却没有对结果有要求。 聚类（clustering）问题 没有标签的情况下，我们是否能给数据分类呢？ 主成分分析（principal component analysis）问题 能否找到少量的参数来准确地捕捉数据的线性相关属性？ 因果关系（causality）和概率图模型（probabilistic graphical models）问题 能否描述观察到的许多数据的根本原因？ 生成对抗性网络（generative adversarial networks） 为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。 与环境互动 到目前为止，不管是监督学习还是无监督学习，我们都会预先获取大量数据，然后启动模型，不再与环境交互。这里所有学习都是在算法与环境断开后进行的，被称为 离线学习（offline learning） 。对于监督学，从环境中收集数据的过程类似于，下图： 好的一面是，我们可以孤立地进行模式识别，而不必分心于其他问题。 缺点是，解决的问题相当有限。 与预测不同，“与真实环境互动”实际上会影响环境。 我们必须考虑到它的行为可能会影响未来的观察结果。 当训练和测试数据不同时，最后一个问题提出了 分布偏移（distribution shift） 的问题。 强化学习 强化学习问题，这是一类明确考虑与环境交互的问题。 使用机器学习开发与环境交互并采取行动。 这可能包括应用到机器人、对话系统，甚至开发视频游戏的人工智能（AI）。 深度强化学习（deep reinforcement learning）将深度学习应用于强化学习的问题，是非常热门的研究领域。 深度Q网络（Q-network）在雅达利游戏中仅使用视觉输入就击败了人类， AlphaGo 程序在棋盘游戏围棋中击败了世界冠军。 原理：在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体从环境接收一些观察（observation），并且必须选择一个动作（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得奖励（reward）。 强化学习的 目标是产生一个好的策略（policy） 。 强化学习智能体选择的“动作”受策略控制，即一个从环境观察映射到行动的功能。 强化学习框架的 通用性十分强大 。例如，我们可以将任何监督学习问题转化为强化学习问题。假设我们有一个分类问题，可以创建一个强化学习智能体，每个分类对应一个“动作”。然后，我们可以创建一个环境，该环境给予智能体的奖励。 强化学习可以解决许多监督学习无法解决的问题。例如，在监督学习中，我们总是希望输入与正确的标签相关联。但在强化学习中，我们并不假设环境告诉智能体每个观测的最优动作。一般来说，智能体只是得到一些奖励。此外，环境甚至可能不会告诉是哪些行为导致了奖励。 强化学习者必须处理学分分配（credit assignment）问题：决定哪些行为是值得奖励的，哪些行为是需要惩罚的。 以强化学习在国际象棋的应用为例。 唯一真正的奖励信号出现在游戏结束时：当智能体获胜时，智能体可以得到奖励1；当智能体失败时，智能体将得到奖励-1。 强化学习可能还必须 处理部分可观测性问题 。也就是说，当前的观察结果可能无法阐述有关当前状态的所有信息。 最后，在任何时间点上，强化学习智能体可能知道一个好的策略，但可能有许多更好的策略从未尝试过的。强化学习智能体必须不断地做出选择：是应该利用当前最好的策略，还是探索新的策略空间（放弃一些短期回报来换取知识）。","tags":["机器学习","各类及其学习问题","监督学习","无监督学习","强化学习","Dive into deep learning"],"categories":["人工智能"]},{"title":"机器学习中的三大组件","path":"/2025/05/26/机器学习中的三大组件/","content":"机器学习中的三大组件数据 数据集：由 样本（example, sample） 组成，遵循 独立同分布(independently and identically distributed, i.i.d.) ， 独立性（Independence）​​ 指任意两个随机变量之间互不影响，数学上表现为联合概率等于各变量边际概率的乘积。 例如抛硬币时，每次结果不会影响下一次结果。 ​同分布性（Identical Distribution）​​ 所有随机变量服从完全相同的概率分布，包括参数（如均值、方差）、分布函数（如正态分布、泊松分布）等。例如用同一枚骰子多次投掷，每次结果的概率分布相同。 训练数据集 用于 拟合 模型参数，测试数据集 用于 评估 拟合的模型。 样本有时也叫做 数据点（data point）或者数据实例（data instance） ，通常每个样本由一组称为 特征（features，或协变量（covariates）） 的属性组成。机器学习模型会根据这些属性进行预测。 若要预测的是一个特殊的属性，它被称为 标签（label，或目标（target）） 。 模型 深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为 深度学习（deep learning） 。 目标函数 在机器学习中，我们需要定义模型的优劣程度的度量，这个度量在大多数情况是“可优化”的，这被称之为 目标函数（objective function） 。 我们通常定义一个目标函数，并希望优化它到最低点。 因为越低越好，所以这些函数有时被称为 损失函数（loss function，或cost function） 。 损失函数是根据模型参数定义的，并取决于数据集。 在一个数据集上，可以通过 最小化总损失 来学习模型参数的最佳值。该数据集由一些为训练而收集的样本组成，称为 训练数据集（training dataset，或称为训练集（training set）） 。 我们也可以取一个新的函数，优化到它的最高点。 这两个函数本质上是相同的，只是翻转一下符号。 预测 试图 预测数值 时，最常见的损失函数是 平方误差（squared error） ，即 预测值与实际值之差的平方 。 试图 解决分类问题 时，最常见的目标函数是最小化错误率，即 预测与实际情况不符的样本比例 。 有些目标函数（如平方误差）很容易被优化，有些目标（如错误率）由于 不可微性或其他复杂性 难以直接优化。 在这些情况下，通常 会优化替代目标 。 当一个模型在训练集上表现良好，但不能推广到测试集时，这个模型被称为 过拟合（overfitting） 的。 优化算法 目的： 当我们获得了一些数据源及其表示、一个模型和一个合适的损失函数，接下来就需要一种算法，它能够搜索出最佳参数，以最小化损失函数 。 方法： 深度学习中，大多流行的优化算法通常基于一种基本方法 梯度下降（gradient descent） ：简而言之，在每个步骤中，梯度下降法都会检查每个参数，看看如果仅对该参数进行少量变动，训练集损失会朝哪个方向移动。然后，它在可以减少损失的方向上优化参数。","tags":["人工智能","机器学习","数据","模型","目标函数","优化算法"],"categories":["人工智能"]},{"title":"如何安装Jupyter","path":"/2025/05/26/安装Jupyter/","content":"如何安装Jupyter步骤 安装anaconda 安装jupyter：jupyter的工作目录默认是&#x2F;Users&#x2F;mac_name 安装 12# 安装jupyterpip install jupyter 12# 启动jupyterjupyter notebook","tags":["python","Jupyter"],"categories":["人工智能"]},{"title":"Hexo图片显示问题","path":"/2025/05/25/Hexo_blog图片显示问题/","content":"图片显示问题问题 思考过程1、 通过F12调用，尝试看看图片获取的路径位置,猜测是：放错文件夹。2、 上网搜集信息，关键词：hexo 图片放置。 法一 Hexo Asset Image 专为 Hexo 博客框架设计的开源插件，它旨在简化图片资源的管理和插入流程。通过本插件，博主可以更加便捷地在文章中引用本地或远程的图片，并自动处理图片路径，优化博客的构建和部署过程。 设置： 在_config.yml文件里面设置：该资源文件夹的名称将与文章关联的Markdown文件同名。将所有与文章相关的资源文件（如图片、附件等）统一放入对应的资源文件夹中，即可通过相对路径直接引用这些资源。 1post_asset_folder: true 安装：通过终端，进入blog文件夹，输入安装 Hexo Asset Image ： 1npm install hexo-asset-image --save 配置：编辑Hexo的配置文件 _config.yml，添加以下行以启用插件： 使用 hexo new 新文章，会在 \\source_posts 目录下生成一个 新文章.md，还会生成一个同名的文件夹，这个文件夹下面可以存放该文章的所有资源（例如图片、附件等） 12plugins: - hexo-asset-image 使用 ： ![logo](phah/logo.jpg) 结果：失败分析：无法正确解析 解析无法解析图片的方法(1)1、 在npm中查看hexo-asset-image的使用说明：Just use ![logo](logo.jpg) to insert logo.jpg。 2、 尝试结果：解析失败 结果：失败 解析无法解析图片的方法(2)参考：hexo 博客插入本地图片时遇到的坑 https://zhuanlan.zhihu.com/p/696630232 1、卸载hexo-asset-image：将安装的install改成uninstall1npm uninstall hexo-asset-img --save 2、安装 1npm install hexo-asset-img --save 3、 使用: ![logo](hexo-typora/logo.png) 结果：失败 解析无法解析图片的方法(3)参考：https://blog.csdn.net/kai_99/article/details/125557840 1、 思路：使用法一，然后上传到github，然后访问，便可查看！ 2、 使用&#123;% asset_img 图片.jpg 图片描述 %&#125;而非![](img/img.png) 1![](img/img.png)&#123;% asset_img 图片.jpg 图片描述 %&#125; 结果：失败 解析无法解析图片的方法(4)分析问题参考：https://blog.csdn.net/m0_72761863/article/details/136284105 如图，将&#x2F;.io&#x2F;&#x2F;删除后，出现图片,所以只需要删除&#x2F;.io&#x2F;&#x2F; 成功解决方法修改hexo-asset-image模块下的index.js 将图片中哪一行替换 1var endPos = link.length-1; 成功 参考文章：1、 Hexo Asset Image插件教程 https://blog.csdn.net/gitblog_00584/article/details/141445964 2、 hexo 博客插入本地图片时遇到的坑 https://zhuanlan.zhihu.com/p/696630232","tags":["Hexo","图片问题","图片显示"],"categories":["Hexo"]},{"title":"Tensor中的矩阵分解","path":"/2025/05/25/Tensor中的矩阵分解/","content":"Tensor中的矩阵分解矩阵分解常见的矩阵分解 LU分解：将矩阵A分解成L（下三角）矩阵和U（上三角）矩阵的乘积 “L –&gt; Lower Triangular Matrix” “U –&gt; Upper Triangular Matrix” QR分解：将原矩阵分解成一个正交矩阵Q和一个上三角矩阵R的乘积 EVD分解：特征值分解 SVD分解：奇异值分解 其他矩阵分解 特征值分解 将矩阵分解为由其特征值和特征向量表示的矩阵之积的方法 特征值 VS 特征向量 PCA与特征值分解 PCA：将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征 PCA算法的优化目标就是： 降维后同一纬度的方差最大 不同维度之间的相关性为0 协方差矩阵 奇异值分解 LDA与奇异值分解 EVD分解 VS SVD分解 矩阵方阵且满秩（可对角化） 矩阵分解不等于特征降维度 协方差矩阵描述方差和相关性 Pytorch中的奇异值分解 torch.svd()","tags":["pytorch","Tensor","矩阵","矩阵分解","矩阵分解的方法"],"categories":["人工智能"]},{"title":"统计学相关的函数、随机函数","path":"/2025/05/24/分布函数、随机函数/","content":"统计学相关的函数、随机函数统计学相关的函数 torch.mean() # 返回平均值 torch.sum() # 返回总和 torch.prod() # 计算所有元素的积 torch.max() ＃ 返回最大值 torch.min() ＃ 返回最小值 torch.argmax() # 返回最大值排序的索引值 torch.argmin() # 返回最小值排序的索引值 包含可参数化的概率分布和采样函数（torch.distributions） distributions 包含可参数化的概率分布和采样函数 得分函数 强化学习中策略梯度方法的基础 pathwise derivative估计器 变分自动编码器中的重新参数化技巧 TensoriJtorch.distributions KL Divergence Transforms Constraint ExponentialFamily, Bernoulli, Categorical, Beta, Binomial,Cauchy, Chi2, Dirichlet, Exponential, FisherSnedecor, Gamma, Geometric, Gumbel, HalfCauchy, HalfNormal.Independent, Laplace, LogNormal,LowRankMultivariateNormal, Multinomial,MultivariateNormal,NegativeBinomial, Normal, OneHotCategorical, Pareto,Poisson, RelaxedBernoulli, RelaxedOneHotCategorical,StudentT, TransformedDistribution, Uniform, Weibull 随机抽样定义随机种子+ torch.manual seed（seed） 定义随机数满足的分布+ torch.normal() 范数+ 在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即①非负性；②齐次性；③三角不等式。 + 常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。 o范数&#x2F;1范数&#x2F;2范数&#x2F;p范数&#x2F;核范数+ torch.dist(input, other,p=2) # 汁算p范数 + torch.norm() # 计算2范数","tags":["pytorch","Tensor","函数","统计学相关的函数","随机函数"],"categories":["人工智能"]},{"title":"Tensor的函数","path":"/2025/05/24/Tensor的三角函数/","content":"Tensor的函数三角函数 torch.abs(input, out&#x3D;None) # 计算输入张量中每个元素的绝对值。 torch.cos(input, out&#x3D;None) # 计算输入张量中每个元素的余弦值，输入为弧度制，输出范围 ​[-1, 1]。 torch.acos(input, out&#x3D;None) # 计算反余弦和反正弦值，输出为弧度制。 torch.cosh(input, out&#x3D;None) # 计算双曲余弦和双曲正弦值。 torch.sin(input, out&#x3D;None) # 计算输入张量中每个元素的正弦值（输入为弧度制），输出范围 [-1, 1]。 torch.asin(input, out&#x3D;None) # 计算输入张量的反正弦值（输出为弧度制），输入必须在 [-1, 1] 范围内。 torch.sinh(input, out&#x3D;None) # 计算输入张量的双曲正弦值，数学定义为 sinh(x) &#x3D; (e^x - e^(-x)) &#x2F; 2 torch.tan(input, out&#x3D;None) # 计算正切值，输入为弧度制。 torch.atan(input, out&#x3D;None) # 计算输入张量中每个元素的反正切值（单参数版本），输出范围为 ​[-π&#x2F;2, π&#x2F;2]​。 torch.atan2(input, inpu2,out&#x3D;None) # 根据两个输入参数 input1（y）和 input2（x）计算反正切值，输出范围为 ​[-π, π]，能识别象限。 torch.tanh(input, out&#x3D;None) # 计算双曲正切，输出范围 (-1, 1)。 其他的数学函数 torch.abs() # 绝对值计算​ torch.erf() # 误差函数​ torch.erfinv() # 逆误差函数​ torch.sigmoid() # S 型激活函数​ torch.neg() # 数值取反​ torch.reciprocal() # 倒数计算​ torch.rsqrt() # 平方根倒数​ torch.sign() # 符号函数​ torch.lerp() # 线性插值 torch.addcdiv() # 标量加权除法加法​ torch.addcmul() # 加权逐元素乘法加法​ torch.cumprod() # 累积乘积​ torch.cumsum() # 累积和​","tags":["pytorch","Tensor","函数","三角函数","其他的数学函数"],"categories":["人工智能"]},{"title":"Tensor的基本运算","path":"/2025/05/24/Tensor的基本运算/","content":"Tensor的基本运算取整&#x2F;取余运算 .floor（向下取整数） 地板函数 .ceil（向上取整数） 天花板函数 .round（四舍五入&gt;&#x3D;0.5向上取整，&lt;0.5向下取整） .trunc（裁剪，只取整数部分）“truncation” .frac（只取小数部分）“fracture” %取余 比较运算 torch.eq（input, other, out&#x3D;None）#按成员进行等式操作，相同返回True torch.equal（tensor1, tensor2） #如果tensor1和tensor2有相同的size和elements, 则为true eq与equal的区别： eq：逐元素比较两个张量（或张量与标量）是否相等。 equal：全局判断两个张量是否完全一致（形状相同且所有元素相等）。 torch.ge(input, other, out&#x3D;None) #input&gt;&#x3D;other “​Greater than or Equal” torch.gt(input, other, out&#x3D;None) # input&gt;other “​Greater than” torch.le(input, other, out&#x3D;None) #input&#x3D;&lt;other “Less than or Equal​” torch.lt(input, other, out&#x3D;None) #input&lt;other “Less Than​” torch.ne(input, other, out&#x3D;None) #input!&#x3D;other “Not Equal​” 取前k小&#x2F;第k小的数值及其索引 torch.sort(input, dim&#x3D;None, descending&#x3D;False, out&#x3D;None) #对目标input进行排序 torch.topk（input, k, dim&#x3D;None, largest&#x3D; True, sorted &#x3D;True,out&#x3D;None）#沿着指定维度返回最大k个数值及其索引值 “​Top K Values​” torch.kthvalue(input, k, dim&#x3D;None, out&#x3D;None) #沿着指定维度返回第k个最小值及其索引值 “​K-th Smallest Value​” 判定是否为finite&#x2F;inf&#x2F;nan torch.isfinite(tensor) &#x2F; torch.isinf(tensor) &#x2F; torch.isnan(tensor) 返回一个标记元素是否为 finite&#x2F;inf&#x2F;nan 的mask张量。","tags":["pytorch","Tensor基本运算"],"categories":["人工智能"]},{"title":"in-place 操作和广播机制","path":"/2025/05/24/in-place 和 广播机制/","content":"in-place 操作定义1、 不允许使用临时变量 2、 x = x + y 3、 add_、sub_、mul_等 广播机制定义张量参数可以自动扩展为相同大小。 满足条件1、 每个张量至少有一个维度 2、 满足右对齐 3、 eg： torch.rand(2,1,1)+torch.rand(3) - A: (2, 1, 1) - B: (3,) → 自动补为 (1, 1, 3) 注： 右对齐： 从张量形状（shape）的最右侧维度开始，逐一向左比对维度大小，确保维度兼容后才能进行运算。\\ 满足右对齐的原则： ​右对齐后，每个维度大小相等或其中一个为1​ ​所有维度都满足上述条件​ ps：为什么维度1可以扩展成4，而3不行？​​ 当某个维度大小为1时，表示该维度仅有一个元素。扩展时只需复制该元素到目标维度大小，逻辑明确且无歧义。 如果维度大小为3的张量要扩展成4，必须回答：​第4个元素应该是什么？​​ 是复制第一个元素？还是插值？或者是随机填充？ 这种操作没有统一规则，会导致计算结果不可预测。","tags":["pytorch","in-place操作","广播机制"],"categories":["人工智能"]},{"path":"/2025/05/20/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":["hexo"],"categories":["生活"]}]